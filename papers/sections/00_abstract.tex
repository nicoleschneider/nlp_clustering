\begin{abstract}

Text clustering methods traditionally rely on a shared vocabulary and script, which poses a challenge for cross-lingual text clustering problems, which are relevant in a variety of domains including social media, news, finance, and more.
Recent approaches to cross-lingual clustering have found success leveraging latent embedding space representations of neural models and more recently Large Language Models (LLMs) have been used to do text clustering in zero-shot or few-shot settings.

We use cross-lingual clustering in the news domain as a case study to test whether LLMs can effectively extract keyphrases that are useful in cross-lingual clustering, outperform baseline clustering methods in a single language setting, and perform cluster correction in a cross-lingual setting.
We find \nrscomment{summarize findings here}
These findings pave the way for future work in \nrscomment{fill in FW here}


\end{abstract}