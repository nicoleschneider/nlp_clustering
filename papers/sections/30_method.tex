\section{Method}\label{section:method}

In this section we describe our experimental setup, including the baselines for comparison and the prompts we use to elicit cluster assignments from the LLMs.
All clustering is done in an online fashion to simulate a live environment where news articles are arriving as they are published, in real time.
For the single language setting, we use \emph{NewsStandEN}, which contains 100 English articles with hand labeled ground truth cluster assignments.


\subsection{Baseline Clustering: NewsStand}
In the news domain, clustering is used to group together \emph{story clusters} containing all news articles that describe the same news event. 
In addition to the requirement that articles in the same cluster share many of the same keywords, they also must be published around the same timeframe. 
The temporal requirement stems from the emphasis on recency when presenting breaking stories to users. 
This premise lends itself well to online clustering, which requires less computation than one-shot approaches that involve re-clustering the entire corpus with every new article ingested~\cite{Teit08}.

To accomplish the clustering, NewsStand employs the vector space model \cite{salton}, a common approach in text mining and information retrieval. 
If the articles are not originally written in English, they are first translated into English using machine translation.
Then, the articles are converted to term feature vectors in $d$-dimensional space, where $d$ is the number of distinct terms in every document in a corpus. 
The term feature vector is extracted using TF-IDF \cite{salton-buckley}. 
Elements of the term feature vector represent the frequency of their corresponding term in the document being ingested, where terms that are common in a document but uncommon in the corpus are emphasized. 
Emphasis is given to location entities and to terms that appear earlier in the article. 
Since NewsStand is an online system with a dynamic corpus, the term feature vector is computed once for each article at the time it is ingested into the system.

Clustering is also done in an online fashion using a variant of leader-follower clustering \cite{Duda73}. 
Articles are clustered across two dimensions: the term vector space and the temporal dimension. 
A term centroid and a time centroid are maintained for each cluster, representing the mean term feature vector and mean publication time of the articles in the cluster, respectively. 
For each new article ingested, clustering proceeds by checking if there exists a cluster with centroids less than a fixed cutoff distance from the article's term and time values. 
If so, the article is added to the nearest cluster and its centroids are updated, and if not, a new cluster is created containing only the new article. 
Term distances are computed using the standard cosine similarity \cite{steinbach}, and a Gaussian attenuator is applied to the temporal dimension to favor clusters with time centroids near the article's publication time.



\subsection{LLM Clustering}

We compare the baseline clustering method to an LLM-based clustering method, where LLMs are directly prompted to cluster articles.
On the English-only dataset, for which we have ground-truth cluster labels, we test several prompt variants to determine whether including article titles, timestamps, and geo-coordinates enable LLMs to achieve better clustering results compared to the baseline clustering method.
The outcome of those experiments determine the prompts used to conduct clustering on the multi-lingual dataset, for which we do not have ground truth cluster labels.

To achieve the online clustering effect through zero-shot LLM prompting, we follow the LLM prompting strategy in \citeauthor{Schneider2024c}~\cite{Schneider2024c} and provide an initial system prompt stating the task, and then a series of prompts containing individual articles that should be assigned to a cluster.
To retain the context of the previous articles that have already been assigned clusters, we prompt the LLMs to output both article ids and keywords associated with each cluster at each step, and these are fed into the subsequent prompt to accumulate the full clustering of all the articles over the course of each experiment.
For experiments that include geo-coordinates and timestamps, we maintain the average geo-coordinate and/or timestamp of the articles in each cluster and also provide that to the LLM to enable location-based and time-based clustering.

\subsubsection{Models}

\begin{table}[ht!]
    \centering
    \begin{tabular}{l|lr}
        \textbf{Developer} & \textbf{Model} & \textbf{\$ per M/Tok} \\
        \hline
        OpenAI  & gpt-3.5-turbo     & 00.50/01.50 \\
        ~       & gpt-4o             & 05.00/15.00 \\
        ~       & gpt-4o mini       & 0.150/0.600 \\
        ~       & ~                 & ~     \\
        Google  & gemini-1.5-flash  & 00.35/01.05 \\
    \end{tabular}
    \caption{Summary of models evaluated.}
    \label{tab:models}
\end{table}

We select 4 models from two of the leading LLM developers. 
Table~\ref{tab:models} summarizes the selected models and indicative cost of use. 
For experimentation, temperature is set to $0$ on each model and where available, constant seed values are set to reduce the impact of randomness in generation.


\subsubsection{Prompts}

\paragraph{\textbf{E1.}}
Do LLMs perform better clustering with domain information in the initial system prompt?

\noindent To test this we compare the clustering results produced when the LLMs are given a simple prompt stating the goal is to cluster articles vs. a more detailed prompt stating the goal is to form clusters that represent news stories.

\begin{lstlisting}[title=Prompt 1: Generic Initial System Prompt]
    You are tasked with clustering articles. Cluster these articles based on their text content. You will be fed articles one by one in the form:

    id: {id}
    
    text: {text}
    
    Respond with a tuple of predicted cluster and a list of keywords associated with the article. If the cluster already exists, update the list of keywords so that it summarizes both the original cluster and the new article. You can use no more than 5 keywords per cluster. If the article does not belong in an existing cluster, create a new one, incrementing the cluster number by 1. The existing clusters are described in the dictionary provided to you after the article.
\end{lstlisting}


\begin{lstlisting}[title=Prompt 2: Detailed Initial System Prompt]
    You are tasked with clustering articles for a news system based on whether they express ideas about the same news story or world event. Cluster these articles based on their text content, with the aim to consistently group together articles about the same news event. You will be fed articles one by one in the form:

    id: {id}
    
    text: {text}
    
    Respond with a tuple of predicted cluster and a list of keywords associated with the article. If the cluster already exists, update the list of keywords so that it summarizes both the original cluster and the new article. You can use no more than 5 keywords per cluster. If the article does not belong in an existing cluster, create a new one, incrementing the cluster number by 1. The existing clusters are described in the dictionary provided to you after the article.
\end{lstlisting}

For \emph{E1}, we first give each LLM Prompt 1 and then feed it articles from \emph{NewsStandEN} one at a time. Along with each article the output of the previous interaction is provided as the context history, to maintain the cluster assignments made previously.
A similar process is repeated using Prompt 2 as the initial system prompt, and the cluster assignments are recorded for each model under each of the two conditions.

\paragraph{\textbf{E2.}}
Do LLMs perform better at clustering when the article title is provided for additional context? 

\noindent To test this we compare the clustering results produced when the LLMs are given article text vs. article text and article title. 

\begin{lstlisting}[title=Prompt 3: Text Alone Prompt]
    id: {id number}
    
    Text: {text here}
    \end{lstlisting}
    \begin{lstlisting}[title=Prompt 4: Title and Text Prompt]
    id: {id number}
    
    Title: {title here}

    Text: {text here}
\end{lstlisting}

For \emph{E2}, we first give each LLM an initial system prompt following whichever of Prompts 1 or 2 were more successful in \emph{E1}.
Then, we feed the LLM articles from \emph{NewsStandEN} one at a time, in one of the two formats specified in Prompts 3 and 4, adjusting lines 3-5 of the initial system prompt to follow Prompt 3 or Prompt 4. 
With Prompt 3, only the article text is provided to aid with clustering.
With Prompt 4, the article title is provided in addition to the article text.
Along with each article the output of the previous interaction is provided as the context history, to maintain the cluster assignments made previously.
The cluster assignments are recorded for each model under each of the two conditions.

    
    % \item E3: Does it do better with or without providing hand-labeled keywords? (Choose to include title or not based on which setting did best in the first test)
    % \begin{lstlisting}[title=Prompt 5: Keywords Prompt]
    % id: {id number}
    
    % Keywords: {keywords here}

    % Text: {text here}
    % \end{lstlisting}
    % \begin{lstlisting}[title=Prompt 6: No Keywords Prompt]
    % ...
    % \end{lstlisting}

\paragraph{\textbf{E3.}}
Do LLMs perform better at clustering when provided with the geo-coordinates associated with news articles? 

\noindent To test this we compare the clustering results produced when the LLMs are given article text vs. article text and article geo-coordinates. 

\begin{lstlisting}[title=Prompt 5: No Geo-coords Prompt]
    id: {id number}

    text: {text here}
\end{lstlisting}

\begin{lstlisting}[title=Prompt 6: Geo-coords Prompt]
    id: {id number}
    
    coordinates: {coords here}

    text: {text here}
    \end{lstlisting}
    



For \emph{E3}, we first give each LLM whichever of Prompts 1 and 2 were more successful in \emph{E1}, and adjust lines 3-5 of the initial system prompt to follow Prompt 5 or Prompt 6.
In the case Prompt 6 is used, we also append an additional sentence indicating that the geo-coordinates associated with each article would be provided (and the format it would appear in).
Then, we feed it articles from \emph{NewsStandEN} one at a time, in one of the two formats specified in Prompts 5 and 6. 
With Prompt 5, only the article text is provided to aid with clustering.
With Prompt 6, the article's associated geo-coordinates are also provided in addition to the article text.
% Along with each article the output of the previous interaction is provided as the context history, to maintain the cluster assignments made previously.
Along with each article, the cluster dictionary produced by the previous interaction is provided as context history, which includes the cluster IDs as keys, the article IDs associated with each cluster, and the keywords associated with each cluster.
For experiments using prompt 6, we additionally compute the average geo-coordinate of each cluster after each response of the model, and include it along with the articles assigned to that cluster.
The final cluster assignments are recorded for each model under each of the two conditions.


\paragraph{\textbf{E4.}}
Do LLMs perform better at clustering when provided with time of publication of news articles? 

\noindent To test this we compare the clustering results produced when the LLMs are given article text vs. article text and article publication timestamp. 


\begin{lstlisting}[title=Prompt 7: No Timestamp Prompt]
    id: {id number}

    text: {text here}
\end{lstlisting}

\begin{lstlisting}[title=Prompt 8: Timestamp Prompt]
    id: {id number}
    
    timestamp: {timestamp here}

    text: {text here}
\end{lstlisting}

For \emph{E4}, we first give each LLM whichever of Prompts 1 and 2 were more successful in \emph{E1}, and adjust lines 3-5 of the initial system prompt to follow Prompt 7 or Prompt 8.
In the case Prompt 8 is used, we also append an additional sentence indicating that the timestamp associated with each article would be provided (and the format it would appear in).
Then, we feed it articles from \emph{NewsStandEN} one at a time, in one of the two formats specified in Prompts 7 and 8. 
With Prompt 7, only the article text is provided to aid with clustering.
With Prompt 8, the article's associated publication timestamp is also provided in addition to the article text.
% Along with each article the output of the previous interaction is provided as the context history, to maintain the cluster assignments made previously.
Along with each article, the cluster dictionary produced by the previous interaction is provided as context history, which includes the cluster IDs as keys, the article IDs associated with each cluster, and the keywords associated with each cluster.
For experiments using Prompt 8, we additionally compute the average timestamp after each response of the model, and include it along with the articles assigned to that cluster.
The final cluster assignments are recorded for each model under each of the two conditions.



\begin{table*}[ht!]
    \centering
    \begin{tabular}{l|l|cccc|r}
        \textbf{Method}       & \textbf{Model}    & \textbf{System Prompt} & \textbf{Title} & \textbf{Timestamp} & \textbf{Geo-coords} &\textbf{NMI} \\
        \hline
        NewsStand (baseline)  & ~                 & ~                      & ~              & ~                  & ~                    & 0.902 \\
        ~                     & ~                 & ~                      & ~              & ~                  & ~                    & ~   \\
        LLM                   & gpt-3.5-turbo     & Generic                & ~              & ~                  & ~                    & 0.778 \\
        ~                     & gpt-4o            & Generic                & ~              & ~                  & ~                    & 0.839 \\
        ~                     & gpt-4o mini       & Generic                & ~              & ~                  & ~                    & 0.850 \\
        ~                     & gemini-1.5-flash  & Generic                & ~              & ~                  & ~                    & 0.827 \\
        ~                     & ~                 & ~                      & ~              & ~                  & ~                    & ~   \\
        ~                     & gpt-3.5-turbo     & Detailed               & ~              & ~                  & ~                    & 0.806 \\
        ~                     & gpt-4o            & Detailed               & ~              & ~                  & ~                    & 0.909 \\
        ~                     & gpt-4o mini       & Detailed               & ~              & ~                  & ~                    & 0.784 \\
        ~                     & gemini-1.5-flash  & Detailed               & ~              & ~                  & ~                    & 0.809 \\
        ~                     & ~                 & ~                      & ~              & ~                  & ~                    & ~   \\
        ~                     & gpt-3.5-turbo     & Generic               & X              & ~                  & ~                    & 0.752 \\
        ~                     & gpt-4o            & Generic               & X              & ~                  & ~                    & 0.832 \\
        ~                     & gpt-4o mini       & Generic               & X              & ~                  & ~                    & 0.803 \\
        ~                     & gemini-1.5-flash  & Generic               & X              & ~                  & ~                    & 0.829 \\
        ~                     & ~                 & ~                      & ~              & ~                  & ~                    & ~   \\
        ~                     & gpt-3.5-turbo     & Generic               & ~              & X                  & ~                    & 0.769 \\
        ~                     & gpt-4o            & Generic               & ~              & X                  & ~                    & 0.595 \\
        ~                     & gpt-4o mini       & Generic               & ~              & X                  & ~                    & 0.836 \\
        ~                     & gemini-1.5-flash  & Generic               & ~              & X                  & ~                    & 0.849 \\
        ~                     & ~                 & ~                      & ~              & ~                  & ~                    & ~   \\
        ~                     & gpt-3.5-turbo     & Generic               & ~              & ~                  & X                    & 0.770 \\
        ~                     & gpt-4o            & Generic               & ~              & ~                  & X                    & 0.788 \\
        ~                     & gpt-4o mini       & Generic               & ~              & ~                  & X                    & 0.815 \\
        ~                     & gemini-1.5-flash  & Generic               & ~              & ~                  & X                    & 0.815 \\

    \end{tabular}
    \caption{Summary of single-language clustering results for baseline and LLM methods compared to ground-truth cluster assignment.}
    \label{tab:single-lang-results}
\end{table*}



\subsubsection{Measurements}
We begin with the single-language setting, using the \emph{NewsStandEN} dataset.
We record the baseline cluster label assignments from the NewsStand system and record the LLM performances for Prompts 1-8.
For these experiments we record the cluster labels assigned to each article by each method or model and using those assignments, we compute the \ac{NMI} between the recorded cluster assignment, $U = \{U_1, ..., U_p\}$, and the ground truth cluster assignment, $V = \{V_1, ..., V_q\}$, of the $N$ data points as follows~\cite{vinh10a}.
%
The normalized mutual information, $NMI(U,V)$, is defined as
\begin{equation}\label{eq:rwd-func}
\begin{split}
    NMI(U, V) = \frac{MI(U, V)}{H(U) + H(V)}
\end{split}
\end{equation}

\noindent where mutual information, $MI(U,V)$, is
\begin{equation}
    MI(U, V) = \sum\limits_{i=1}^p\sum\limits_{j=1}^q \frac{n_{ij}}{N}\log\left(\frac{n_{ij}/N}{\lvert U_i \rvert \lvert V_j \rvert / N^2}\right).
\end{equation}
\noindent for $n_{ij} = \lvert U_i \cap V_j \rvert$ for $1 \leq i \leq p$ and $1 \leq j \leq q$ and where the entropy $H$ of a cluster assignment $Z = \{Z_1, ..., Z_k\}$ is
\begin{equation}
    \begin{split}
        H(Z) = -\sum\limits_{i=1}^{k} \frac{\lvert Z_i \rvert}{N}\log \frac{\lvert Z_i \rvert}{N}.
    \end{split}
\end{equation}


\ac{NMI} is a measure of how much information is shared between two distributions~\cite{Strehl2002}. 
In our context, an \ac{NMI} of 1.0 means that the recorded clustering matches exactly the clustering in the reference dataset and an \ac{NMI} of 0.0 means the two clustering assignments are independent. 
%
We elect to use \ac{NMI} as the measure of clustering success as opposed to alternative measurements like B-Cubed~\cite{bagga1998entity} since \ac{NMI} does not reward grouping outliers into one cluster.
In the news event clustering setting, outliers should each be given their own new cluster, or should be grouped into the nearest related cluster, but should not be all grouped together into an `outlier' cluster, which would not correspond to any coherent news event.

For the cross-lingual setting, we record the baseline cluster label assignments of the \emph{NewsStandMULTI} dataset using the NewsStand system.
Based on the LLM results from the single-language setting, we select the best performing initial system prompt and metadata settings (with or without title, geo-coordinates, and timestamp) to use in the subsequent cross-lingual LLM experiments.
Since there are no ground truth cluster labels for \emph{NewsStandMULTI}, we compare the \ac{NMI} between the LLMs and the baseline method and qualitatively analyze the cluster results of both methods.

% OPTIONAL - low priority:
% \begin{itemize}
%     \item Can it extract key phrases? 
%     \begin{lstlisting}[title=Prompt 10: Keyphrase Extraction Prompt]
%     ...
%     \end{lstlisting}
% \end{itemize}

% Compare the keywords it identifies to our hand-labeled keywords in the data.


% \subsection{Cross-Lingual Cluster Correction}
% OPTIONAL - nice to have. Do this after we have results if we need more content and have more time
% \begin{itemize}
%     \item Give LLM the cluster assignments newsstand came up with for the large dataset and ask it to improve them? 
%     \item For each point it moves, manually evaluate if its a better fit than the original cluster that article belonged to?
%     \item Any trends with the ones it improved vs. the ones it moved to worse clusters? Better at English-original text?
%     \begin{lstlisting}[title=Prompt 11: Cluster Correction Prompt]
%     ...
%     \end{lstlisting}
% \end{itemize}

